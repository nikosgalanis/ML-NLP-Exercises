{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Textual_QA.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20bb31e6f32848688dc3fdc2c92645ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ebd1213227a54b44b6a3cf9bdd62e5fa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ed8aa6e388cd49e8bb09ea7a45801ad3",
              "IPY_MODEL_0bf8af40be924f3e99ab485ec5f523c2"
            ]
          }
        },
        "ebd1213227a54b44b6a3cf9bdd62e5fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed8aa6e388cd49e8bb09ea7a45801ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14335e52fca84e1990deb0e54879fb9a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 26247,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26247,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7da74a0498ff4692ac08a6e02be6d64c"
          }
        },
        "0bf8af40be924f3e99ab485ec5f523c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16aee4a5b24b4b3eaf09ef70e0a595e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26247/26247 [17:11&lt;00:00, 25.45it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cce71ec437a04b5ab34a24ae51b62c96"
          }
        },
        "14335e52fca84e1990deb0e54879fb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7da74a0498ff4692ac08a6e02be6d64c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16aee4a5b24b4b3eaf09ef70e0a595e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cce71ec437a04b5ab34a24ae51b62c96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wdtnxl870KG"
      },
      "source": [
        "# Textual Question Answering for the SQuAD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVrjDedJ77Xr"
      },
      "source": [
        "The goal of this notebook is to build a BERT-based model which returns __an answer__, given a user question and a passage which includes the actual answer to the question. We are going to use the SQuAD 2.0 dataset, and begin our testings based on the bert-base-uncased model. We are going to fine-tune it, and evaluate its behaviour based on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l9X7yRk8dBA"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgTh18VR8fWc",
        "outputId": "9726188a-71e2-4059-ed6d-e97b72a899b6"
      },
      "source": [
        "!pip3 install transformers\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "import tqdm.notebook as tq\n",
        "\n",
        "import torch\n",
        "# for json parsing\n",
        "import json\n",
        "# For data vizualization \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "# For large and multi-dimensional arrays\n",
        "import numpy as np\n",
        "# For basic cleaning and data preprocessing \n",
        "import re\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# fancy progress bar\n",
        "from tqdm import tqdm \n",
        "# time measurements\n",
        "import time\n",
        "# some math\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGR1NKaQ8fn4"
      },
      "source": [
        "## The SQuAD 2.0 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYZ9etAk8iZ0"
      },
      "source": [
        "The SQuAD dataset contains several passages, questions upon them, and their respective answers. It is stored in JSON files, thus we will have to parse them and store them in a pandas dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aocSUiOz56Fe"
      },
      "source": [
        "The usefull info that we want to mine from the dataset is the following:\n",
        " - Each __question__ posed\n",
        " - Each __answer__ to the question (there might be more than one)\n",
        " - The __label__ of each answer, aka the start end the end point in the context\n",
        " - The information if the answer is __impossible__\n",
        " - Each __context__ a.k.a. the paragraphs that answer the questions\n",
        " - The __full text__ in the way that BERT wants it\n",
        "\n",
        "The SQuAD 2.0 dataset contains questions that can not be answered. If that is the case, we append an empty string as an answer, in order to encode this info that will be used for training.\n",
        "\n",
        "The function used in order to parse the SQuAD dataset is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckz6NZZZLh_c"
      },
      "source": [
        "# function to parse the squad dataset\n",
        "def parse_squad(filename):\n",
        "    # dict -> df\n",
        "    data = {'id': [], 'question': [], 'answer': [], 'context': [], 'label': [], 'impossible_answer': []}\n",
        "    # open the file\n",
        "    with open(filename) as f:\n",
        "        # and store its json values\n",
        "        dataset = json.load(f)\n",
        "        # for each article\n",
        "        for article in dataset['data']:\n",
        "            # for each paragraph\n",
        "            for par in article['paragraphs']:\n",
        "                # for each QA\n",
        "                for qa in par['qas']:\n",
        "                    # for each one of the gold answers\n",
        "                    if (qa['is_impossible']):\n",
        "                        data['id'].append(qa['id'])\n",
        "                        data['question'].append(qa['question'])\n",
        "                        data['answer'].append(\"\")\n",
        "                        data['context'].append(par['context'])\n",
        "                        data['label'].append((0,0))\n",
        "                        data['impossible_answer'].append(qa['is_impossible'])\n",
        "\n",
        "                    else:\n",
        "                        for ans in qa['answers']:\n",
        "                            # keep id, question, answer and context\n",
        "                            data['id'].append(qa['id'])\n",
        "                            data['question'].append(qa['question'])\n",
        "                            data['answer'].append(ans['text'])\n",
        "                            data['context'].append(par['context'])\n",
        "                            # we want to store the labels, aka the start and the end of the answer\n",
        "                            qstart = ans['answer_start']\n",
        "                            qend = qstart + len (ans['text'])\n",
        "                            data['label'].append((qstart, qend))\n",
        "                            # does the answer exist?\n",
        "                            data['impossible_answer'].append(qa['is_impossible'])\n",
        "    # create the df and return it\n",
        "    return pd.DataFrame(data)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8laCLZmz9tAj"
      },
      "source": [
        "If we load both of the datasets in the same time, memory crashes. Thus, we are going to load only the train dataframe, in order to fine-tune the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57IatF_MJFh"
      },
      "source": [
        "# train_df = parse_squad(\"/content/drive/MyDrive/tn2/train-v2.0.json\")\n",
        "valid_df = parse_squad(\"/content/drive/MyDrive/tn2/dev-v2.0.json\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUoZwFM9wJD"
      },
      "source": [
        "Let's take a look..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_A5uLUl1dnj"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCXkF_2XvZd7"
      },
      "source": [
        "## Using pre-trained BERT models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2WbnFI8GyZZ"
      },
      "source": [
        "As mentioned in the instructions given, we are going to use the `bert-base-uncased` pre-trained model. We are going to feed it with our data, fine-tune it and then test it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZO6BI6HvbqT"
      },
      "source": [
        "### Creating the tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQPSccTGNSK"
      },
      "source": [
        "The first order of business is to create the tokens to pass to the model. In order to do this, we must load the tokenizer for our model, and pass each row of our dataframe through it. The BERT models accept inputs that are in the form:\n",
        "\n",
        "                                `[CLS] + Question + [SEP] + passage + [SEP]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmfWUo6pAWOy"
      },
      "source": [
        "Load the tokenizer from the model we are going to use, the `bert-base-uncased`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "664jf-jXvfuX"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm25zOcVAeTD"
      },
      "source": [
        "Create the tokens for the train and validation datasets using the tokenizers that we have loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCwId7PBCPVO"
      },
      "source": [
        "# tokenize the datasets\n",
        "train_encoding = tokenizer(list(train_df['question'].values), list(train_df['context'].values), truncation=True, padding=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSvssiuOULtA"
      },
      "source": [
        "Next up, in order to correctly predict the location of the answers, we must insert the labels to our encoded tokens. The labels are currently pointing to characters, thus we must change them in order to point to tokens. We are going to use the `char_to_token` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4LhAgryUWzY"
      },
      "source": [
        "def add_labels(encoding, df):\n",
        "    # list to append the labels\n",
        "    token_labels = []\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        # start position of the answer\n",
        "        start = encoding.char_to_token(i, df.iloc[i]['label'][0], sequence_index=1)\n",
        "        # if it is None, set it to maximum length\n",
        "        if start is None:\n",
        "            start = tokenizer.model_max_length\n",
        "        # end position of the answer\n",
        "        end = encoding.char_to_token(i, df.iloc[i]['label'][1], sequence_index=1)\n",
        "        # if it is None, ajust it to match the start of the question\n",
        "        if end is None:\n",
        "            end = start + len(df.iloc[i]['answer'].split()) \n",
        "        # append to our list\n",
        "        token_labels.append((start, end))\n",
        "    # new column to the tokens\n",
        "    encoding.update({'labels': token_labels})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNaG9ZQcWrsX"
      },
      "source": [
        "Apply the function to our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcYaP8gZVZjN"
      },
      "source": [
        "add_labels(train_encoding, train_df)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v23U2lViYV-E"
      },
      "source": [
        "We are going to use a batch size of `8` for the training. When I tested for bigger batch size, the program ran out of memory, so we are going to run it whith a small one, specifically 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chvnMYOyf86Z"
      },
      "source": [
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encoding)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCc9iupeYR-U"
      },
      "source": [
        "# create the dataloaders\n",
        "train_data = DataLoader(train_dataset, batch_size=8, shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNNXNv2FC11l"
      },
      "source": [
        "### Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5nwmKfC2_-"
      },
      "source": [
        "As we mentioned, we are going to use the `bert-base-uncased` model. This is a pre-trained model, but is not optimized for our dataset. Hence, some weights in the model exist, but we aim to make them better by fine-tunning the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb8BaHsKBgrs"
      },
      "source": [
        "For this purpose, we are going to need an __optimizer__ and a __loss_function__, in order to further train the model. The learning rate hyperparameter that we are going to use is $10^{-5}$. I experimented with different hyperparameters, and came to the conclusion that this produces the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYt77pHTxszW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1327f5-5bd9-4fc4-da8b-84f0dbeccdf1"
      },
      "source": [
        "# define our model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "# use GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# learning rate\n",
        "lr = 0.00001\n",
        "# optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=lr)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXZJu9prNPmo"
      },
      "source": [
        "### Further training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCBA0CSoNTAP"
      },
      "source": [
        "In order to achieve our goal, we are going to feed all of our dataset to the model, in order to train in on the specific questions. We are going to do so, the traditional way: by turing on the train mode, and train for the hyperparameters that we are going to define up next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMb6GLTrNPWj"
      },
      "source": [
        "# transfer model to GPU for faster calculations\n",
        "model.to(device)\n",
        "# enable training mode\n",
        "model.train()\n",
        "\n",
        "# define the number of epochs\n",
        "epochs = 2\n",
        "\n",
        "loss_list = []\n",
        "# for each epoch\n",
        "for epoch in range(epochs):\n",
        "    # for each batch in our data loader\n",
        "    for i, batch in tq.tqdm(enumerate(train_data), total= epochs * len(train_data), position=0, leave=True):\n",
        "        if (i == 1):\n",
        "            break\n",
        "        # reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # get the info from the data loader\n",
        "        ids = batch['input_ids'].to(device)\n",
        "        mask = batch['attention_mask'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        \n",
        "        # forward to our model in order to train it.\n",
        "        res = model.forward(ids, attention_mask=mask, start_positions=labels[:,0], end_positions=labels[:,1], token_type_ids=token_type_ids)\n",
        "        \n",
        "        # the first tensor is the loss\n",
        "        loss = res[0]\n",
        "        if (i == 0):\n",
        "            sum = 0\n",
        "        if (i % 100 == 0 and i > 0):\n",
        "            loss_list.append(sum / 100)\n",
        "            sum = 0\n",
        "\n",
        "        sum += loss.item()\n",
        "        # backprobagate\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_CUAR_yWVgq"
      },
      "source": [
        "#### Plotting the Loss Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlfqzXXMX21s"
      },
      "source": [
        "Let's now plot the loss curve of our model, in order to observe how training proceeded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OcEJl85X8RP"
      },
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"Batch\", fontsize=15)\n",
        "plt.ylabel(\"Loss\", rotation=0, fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "300M39EUYMLg"
      },
      "source": [
        "As expected, the loss declines as the training proceeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnzY03ok4lI2"
      },
      "source": [
        "We are going to save our model, because the training takes extremely long time, and we want to save each state of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TlWgfz_oRbi"
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/tn2/mondelo\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEX6565nYYrp"
      },
      "source": [
        "Load a presaved model. My final model can be found [here](https://drive.google.com/file/d/1hqxek4ZbZavYMbtXSvpjzeAZZ73Ukakd/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2PutAD1qOrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8060a59e-30c7-466f-da83-e909983dc6a9"
      },
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/tn2/mondelo\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEuYLwtimY33"
      },
      "source": [
        "Now, we can delete the training data in order to save some space in our memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWPn9EW1mfrR"
      },
      "source": [
        "del train_encoding\n",
        "del train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGH5Gn2CREVD"
      },
      "source": [
        "### Evaluating the validation dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBCMDwiOXxn-"
      },
      "source": [
        "Finally, we are going to use the validation dataset in order to observe the difference made after fine-tunning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix6Zj1GaRyiw"
      },
      "source": [
        "# Function for producing the answer string (from the lectures, altered)\n",
        "def answer(input_ids, tokenizer, ans_start, ans_end):\n",
        "    # if starting and ending tokens are the same, that means that there is no answer to the question\n",
        "    if (ans_start == ans_end):\n",
        "        return ''\n",
        "    # convert the tokens to strings\n",
        "    token_ids = input_ids.tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "    # avoid an overflow\n",
        "    if (ans_start >= len(tokens)):\n",
        "        return ''\n",
        "    # create the answer by appending the tokens\n",
        "    answer = tokens[ans_start]\n",
        "    for token_index in range(ans_start + 1, ans_end + 1):\n",
        "        if tokens[token_index][0:2] == '##':\n",
        "            answer += tokens[token_index][2:]\n",
        "        else:\n",
        "            answer += ' ' + tokens[token_index]\n",
        "    return answer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QPSeZILm7Ev"
      },
      "source": [
        "Now we are going define the evaluation function, that we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMLV3UyZZQNj"
      },
      "source": [
        "def evaluate(model, valid_data):\n",
        "    # evaluation mode\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # dictionary for the answers\n",
        "    answers = {}\n",
        "\n",
        "    # do not update the gradients\n",
        "    with torch.no_grad():\n",
        "        for i, batch in tq.tqdm(enumerate(valid_data), total= len(valid_data), position=0, leave=True):\n",
        "\n",
        "            # get the info from the data loader\n",
        "            id = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "            token_type = batch['token_type_ids'].to(device)\n",
        "\n",
        "            # predict the label by forwarding the data to the model\n",
        "            res = model(id, attention_mask=mask, token_type_ids=token_type)\n",
        "            \n",
        "            # argmax to get the starting and ending positions\n",
        "            ans_start = torch.argmax(res.start_logits).item()\n",
        "            ans_end = torch.argmax(res.end_logits).item()\n",
        "            \n",
        "            # append the answer to the dict\n",
        "            answers[valid_df.iloc[i]['id']] = answer(id[0], tokenizer, ans_start, ans_end)\n",
        "\n",
        "    return answers\n",
        " "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEkfT8Ppkyb9"
      },
      "source": [
        "answers = evaluate(model, valid_data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Flvm4IiVpY"
      },
      "source": [
        "### Altered evaluation script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu9OZxIXiYeQ"
      },
      "source": [
        "We are going to use the evaluation script provided by SQuAD, but we are going to alter it a bit in order to suite us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Ufp8DugJ5z"
      },
      "source": [
        "import string\n",
        "import collections\n",
        "def make_qid_to_has_ans(dataset):\n",
        "  qid_to_has_ans = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid_to_has_ans[qa['id']] = bool(qa['answers'])\n",
        "  return qid_to_has_ans\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def get_raw_scores(dataset, preds):\n",
        "  exact_scores = {}\n",
        "  f1_scores = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid = qa['id']\n",
        "        gold_answers = [a['text'] for a in qa['answers']\n",
        "                        if normalize_answer(a['text'])]\n",
        "        if not gold_answers:\n",
        "          # For unanswerable questions, only correct answer is empty string\n",
        "          gold_answers = ['']\n",
        "        if qid not in preds:\n",
        "          print('Missing prediction for %s' % qid)\n",
        "          continue\n",
        "        a_pred = preds[qid]\n",
        "        # Take max over all gold answers\n",
        "        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
        "        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
        "  return exact_scores, f1_scores\n",
        "\n",
        "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
        "  new_scores = {}\n",
        "  for qid, s in scores.items():\n",
        "    pred_na = na_probs[qid] > na_prob_thresh\n",
        "    if pred_na:\n",
        "      new_scores[qid] = float(not qid_to_has_ans[qid])\n",
        "    else:\n",
        "      new_scores[qid] = s\n",
        "  return new_scores\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
        "  if not qid_list:\n",
        "    total = len(exact_scores)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "  else:\n",
        "    total = len(qid_list)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "\n",
        "def merge_eval(main_eval, new_eval, prefix):\n",
        "  for k in new_eval:\n",
        "    main_eval['%s_%s' % (prefix, k)] = new_eval[k]\n",
        "\n",
        "def plot_pr_curve(precisions, recalls, out_image, title):\n",
        "  plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n",
        "  plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlim([0.0, 1.05])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.title(title)\n",
        "  plt.savefig(out_image)\n",
        "  plt.clf()\n",
        "\n",
        "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "                               out_image=None, title=None):\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  true_pos = 0.0\n",
        "  cur_p = 1.0\n",
        "  cur_r = 0.0\n",
        "  precisions = [1.0]\n",
        "  recalls = [0.0]\n",
        "  avg_prec = 0.0\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid_to_has_ans[qid]:\n",
        "      true_pos += scores[qid]\n",
        "    cur_p = true_pos / float(i+1)\n",
        "    cur_r = true_pos / float(num_true_pos)\n",
        "    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n",
        "      # i.e., if we can put a threshold after this point\n",
        "      avg_prec += cur_p * (cur_r - recalls[-1])\n",
        "      precisions.append(cur_p)\n",
        "      recalls.append(cur_r)\n",
        "  if out_image:\n",
        "    plot_pr_curve(precisions, recalls, out_image, title)\n",
        "  return {'ap': 100.0 * avg_prec}\n",
        "\n",
        "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, out_image_dir):\n",
        "  if out_image_dir and not os.path.exists(out_image_dir):\n",
        "    os.makedirs(out_image_dir)\n",
        "  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
        "  if num_true_pos == 0:\n",
        "    return\n",
        "  pr_exact = make_precision_recall_eval(\n",
        "      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_exact.png'),\n",
        "      title='Precision-Recall curve for Exact Match score')\n",
        "  pr_f1 = make_precision_recall_eval(\n",
        "      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_f1.png'),\n",
        "      title='Precision-Recall curve for F1 score')\n",
        "  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
        "  pr_oracle = make_precision_recall_eval(\n",
        "      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_oracle.png'),\n",
        "      title='Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)')\n",
        "  merge_eval(main_eval, pr_exact, 'pr_exact')\n",
        "  merge_eval(main_eval, pr_f1, 'pr_f1')\n",
        "  merge_eval(main_eval, pr_oracle, 'pr_oracle')\n",
        "\n",
        "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
        "  if not qid_list:\n",
        "    return\n",
        "  x = [na_probs[k] for k in qid_list]\n",
        "  weights = np.ones_like(x) / float(len(x))\n",
        "  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
        "  plt.xlabel('Model probability of no-answer')\n",
        "  plt.ylabel('Proportion of dataset')\n",
        "  plt.title('Histogram of no-answer probability: %s' % name)\n",
        "  plt.savefig(os.path.join(image_dir, 'na_prob_hist_%s.png' % name))\n",
        "  plt.clf()\n",
        "\n",
        "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
        "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "  cur_score = num_no_ans\n",
        "  best_score = cur_score\n",
        "  best_thresh = 0.0\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid not in scores: continue\n",
        "    if qid_to_has_ans[qid]:\n",
        "      diff = scores[qid]\n",
        "    else:\n",
        "      if preds[qid]:\n",
        "        diff = -1\n",
        "      else:\n",
        "        diff = 0\n",
        "    cur_score += diff\n",
        "    if cur_score > best_score:\n",
        "      best_score = cur_score\n",
        "      best_thresh = na_probs[qid]\n",
        "  return 100.0 * best_score / len(scores), best_thresh\n",
        "\n",
        "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "  main_eval['best_exact'] = best_exact\n",
        "  main_eval['best_exact_thresh'] = exact_thresh\n",
        "  main_eval['best_f1'] = best_f1\n",
        "  main_eval['best_f1_thresh'] = f1_thresh\n",
        "\n",
        "def compute_metrics(dataset, preds):\n",
        "  \n",
        "  na_probs = {k: 0.0 for k in preds}\n",
        "  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
        "  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
        "  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans, 0)\n",
        "                                        # OPTS.na_prob_thresh)\n",
        "  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans, 0)\n",
        "                                    #  OPTS.na_prob_thresh)\n",
        "  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
        "  if has_ans_qids:\n",
        "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
        "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
        "  if no_ans_qids:\n",
        "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
        "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
        "    \n",
        "    \n",
        "  return out_eval\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JakYHWtzjVAk"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLFA1o5snrtC"
      },
      "source": [
        "Fiest up, we must process the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pColXxJtnhw5"
      },
      "source": [
        "# location of the validation dataset\n",
        "valid_file = \"/content/drive/MyDrive/tn2/dev-v2.0.json\"\n",
        "\n",
        "# open the dataset file\n",
        "with open(valid_file) as f:\n",
        "    # and store its json values\n",
        "    dataset = json.load(f)\n",
        "\n",
        "valid_df = parse_squad(valid_file)\n",
        "# tokenization and addition of labels\n",
        "valid_encoding = tokenizer(list(valid_df['question'].values), list(valid_df['context'].values), truncation=True, padding=True)\n",
        "add_labels(valid_encoding, valid_df)\n",
        "\n",
        "# creation of the data loader\n",
        "valid_dataset = SquadDataset(valid_encoding)\n",
        "valid_data = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSz_RYDyn4NR"
      },
      "source": [
        "Compute the metrics based on the validation script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btV0VVC-ezVI",
        "outputId": "7368174c-0c30-4c51-b6a2-f31ed6622cf4"
      },
      "source": [
        "result = final(dataset['data'], answers)\n",
        "\n",
        "print(\"Exact matches: \", result['exact'])\n",
        "print(\"F1 score: \", result['f1'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exact matches:  49.288301187568436\n",
            "F1 score:  61.47234198490996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5UjZsw0kVLb"
      },
      "source": [
        "### Comparison without fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biUenlMXkYnc"
      },
      "source": [
        "Finally, we are going to see what difference our fine tunning made to the predictions. We are going to re-load the untrained model, and predict the dataset through it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "20bb31e6f32848688dc3fdc2c92645ec",
            "ebd1213227a54b44b6a3cf9bdd62e5fa",
            "ed8aa6e388cd49e8bb09ea7a45801ad3",
            "0bf8af40be924f3e99ab485ec5f523c2",
            "14335e52fca84e1990deb0e54879fb9a",
            "7da74a0498ff4692ac08a6e02be6d64c",
            "16aee4a5b24b4b3eaf09ef70e0a595e2",
            "cce71ec437a04b5ab34a24ae51b62c96"
          ]
        },
        "id": "bt5HNQvRk1d1",
        "outputId": "f347d24e-32b7-4a78-b4dd-4f79b3b2a98f"
      },
      "source": [
        "base_model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "answers = evaluate(base_model, valid_data)\n",
        "\n",
        "result = final(dataset['data'], answers)\n",
        "\n",
        "print(\"Exact matches: \", result['exact'])\n",
        "print(\"F1 score: \", result['f1'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20bb31e6f32848688dc3fdc2c92645ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=26247.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Exact matches:  0.6569527499368315\n",
            "F1 score:  3.3973115115380033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lE5pBVlQIp"
      },
      "source": [
        "As we can see, despite our scores not being exrtemely high, it definately beats the predictions without fine-tunning the model."
      ]
    }
  ]
}
