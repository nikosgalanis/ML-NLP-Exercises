{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_NNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGu2H2gmZo8v"
      },
      "source": [
        "# Sentiment Classifier using feed-forward Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ_-Jo8mZwpR"
      },
      "source": [
        "The goal of this notebook is to develop a sentiment classifier using feed-forward neural networks for the Twitter sentiment analysis dataset. We are going to experiment and develop several models, and compare them over known metrics such as F1 score, Recall and Precision. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWHUFYlMaHDr"
      },
      "source": [
        "We are going to expirement with parameters like:\n",
        " - Number of hidden layers\n",
        " - Activation functions\n",
        " - The loss function\n",
        " - The optimizer\n",
        "\n",
        " The data is going to be preprocessed and scaled, in order to maximize our classifier's potential.\n",
        "\n",
        " The classifier is going to be implemented using the PyTorch API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh9ww9P3abAG"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoAd1jDdZnhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060f71a4-f22e-466f-bc0e-d050e456b02e"
      },
      "source": [
        "!python -m textblob.download_corpora\n",
        "\n",
        "# For NN models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "# For data vizualization \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "# For large and multi-dimensional arrays\n",
        "import numpy as np\n",
        "# For basic cleaning and data preprocessing \n",
        "import re\n",
        "# For data manipulation and analysis\n",
        "import pandas as pd\n",
        "# Stop words\n",
        "import nltk \n",
        "nltk.download('stopwords')  \n",
        "#Data Preprocessing and Feature Engineering\n",
        "from textblob import TextBlob\n",
        "import scipy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# Necessary for data format\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Word embedings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
        "# Machine learning model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support \n",
        "# Best parameters during classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Validation of the model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-4A7wuabKBH"
      },
      "source": [
        "## Dataset Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQAEXggCbZwQ"
      },
      "source": [
        "We are going to import the dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwJe2AHPbAX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d409975b-c253-4556-c85f-3494c19f019f"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/tn2/SentimentTweets.csv\")\n",
        "df.info()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1280000 entries, 0 to 1279999\n",
            "Data columns (total 7 columns):\n",
            " #   Column      Non-Null Count    Dtype \n",
            "---  ------      --------------    ----- \n",
            " 0   Unnamed: 0  1280000 non-null  int64 \n",
            " 1   target      1280000 non-null  int64 \n",
            " 2   id          1280000 non-null  int64 \n",
            " 3   date        1280000 non-null  object\n",
            " 4   flag        1280000 non-null  object\n",
            " 5   user        1280000 non-null  object\n",
            " 6   text        1280000 non-null  object\n",
            "dtypes: int64(3), object(4)\n",
            "memory usage: 68.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6oEEGB2bbDY"
      },
      "source": [
        "...and take a look at its columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBF1iblKbdVL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5b3d87c9-7441-44e6-9ac0-ab9f29e5d587"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>680949</td>\n",
              "      <td>0</td>\n",
              "      <td>2249621587</td>\n",
              "      <td>Fri Jun 19 22:41:08 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>sukumarpant</td>\n",
              "      <td>#brokenpromises...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>406741</td>\n",
              "      <td>0</td>\n",
              "      <td>2059003515</td>\n",
              "      <td>Sat Jun 06 16:03:21 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>MTMSparrow</td>\n",
              "      <td>David Carradine  so sad. Thai's law not sure i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1337108</td>\n",
              "      <td>4</td>\n",
              "      <td>2017466467</td>\n",
              "      <td>Wed Jun 03 08:26:14 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>itsmemcee</td>\n",
              "      <td>A @ 415 B @ 425. Tell your bro i say congrats!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1560887</td>\n",
              "      <td>4</td>\n",
              "      <td>2186457254</td>\n",
              "      <td>Mon Jun 15 18:52:04 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jdfreivald</td>\n",
              "      <td>@littlefluffycat  Indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1466295</td>\n",
              "      <td>4</td>\n",
              "      <td>2064458395</td>\n",
              "      <td>Sun Jun 07 06:19:20 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>CrazyHan</td>\n",
              "      <td>Completed Race 4 Life in 58mins with girlies f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               text\n",
              "0      680949  ...                                #brokenpromises... \n",
              "1      406741  ...  David Carradine  so sad. Thai's law not sure i...\n",
              "2     1337108  ...    A @ 415 B @ 425. Tell your bro i say congrats! \n",
              "3     1560887  ...                          @littlefluffycat  Indeed.\n",
              "4     1466295  ...  Completed Race 4 Life in 58mins with girlies f...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6PI7THWwqzr"
      },
      "source": [
        "Also, because the tweets are more than a million, we are going to keep 10% of them, to avoid overfitting, and to relief our RAM!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uJM3Ll4llu2"
      },
      "source": [
        "df = df[:200000]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrNDekieHP_s"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNOJ9P3xGBCf"
      },
      "source": [
        "Like always, we have to preprocess our data set. We are going to convert everything to lowercase, and remove any punctuation points, weird characters and links.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x7Yolm6hLHv"
      },
      "source": [
        "We are going to classify tweets. As we all know, twitter users use the symbols `#` and `@` a lot. This may cause problem to the classifier, thus we are going to remove them, using `TextBlob`. We are also going to lemmatize the data in order to avoid multiple representations of the same word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfpQQXW8kBVG"
      },
      "source": [
        "def lemmatization(text):\n",
        "  lem = WordNetLemmatizer()\n",
        "  normalized_tweet = []\n",
        "  for word in text:\n",
        "    normalized_text = lem.lemmatize(word,'v')\n",
        "    normalized_tweet.append(normalized_text)\n",
        "  return normalized_tweet"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7fbcC1uGGO1"
      },
      "source": [
        "def text_normalization(text):\n",
        "    # convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # first group of special chars: \\u followed by a number\n",
        "    text = re.sub('u\\d\\w+', '', text)\n",
        "    # remove links\n",
        "    text = re.sub(r'^http?://', ' ', text)\n",
        "    text = re.sub(r'^www://', ' ', text)\n",
        "    # apply blob techniques\n",
        "    tweet_blob = TextBlob(text)\n",
        "    # return normalized text\n",
        "    blob_res = ' '.join(tweet_blob.words)\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    result = ' '.join(\n",
        "        lemmatizer.lemmatize(term) \n",
        "        for term in text.split()\n",
        "    )\n",
        "    return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvQUHNZ2GLEs"
      },
      "source": [
        "def preprocess(input_df):\n",
        "    # Remove rows with missing values in column col\n",
        "    input_df.dropna(inplace=True)\n",
        "    # Speed up code using numpy vectorization\n",
        "    vfunc = np.vectorize(text_normalization)\n",
        "    input_df.text = vfunc(input_df.text.values)\n",
        "    # return processed input_df\n",
        "    return input_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjRCFe45GM25"
      },
      "source": [
        "Let's now apply those techniques in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nobCV3ntGNvK",
        "outputId": "78784252-3654-4eb4-8c5f-06bea69dec81"
      },
      "source": [
        "df = preprocess(df)\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>680949</td>\n",
              "      <td>0</td>\n",
              "      <td>2249621587</td>\n",
              "      <td>Fri Jun 19 22:41:08 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>sukumarpant</td>\n",
              "      <td>#brokenpromises...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>406741</td>\n",
              "      <td>0</td>\n",
              "      <td>2059003515</td>\n",
              "      <td>Sat Jun 06 16:03:21 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>MTMSparrow</td>\n",
              "      <td>david carradine so sad. thai's law not sure if...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1337108</td>\n",
              "      <td>4</td>\n",
              "      <td>2017466467</td>\n",
              "      <td>Wed Jun 03 08:26:14 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>itsmemcee</td>\n",
              "      <td>a @ 415 b @ 425. tell your bro i say congrats!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1560887</td>\n",
              "      <td>4</td>\n",
              "      <td>2186457254</td>\n",
              "      <td>Mon Jun 15 18:52:04 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jdfreivald</td>\n",
              "      <td>@littlefluffycat indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1466295</td>\n",
              "      <td>4</td>\n",
              "      <td>2064458395</td>\n",
              "      <td>Sun Jun 07 06:19:20 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>CrazyHan</td>\n",
              "      <td>completed race 4 life in 58mins with girlies f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               text\n",
              "0      680949  ...                                 #brokenpromises...\n",
              "1      406741  ...  david carradine so sad. thai's law not sure if...\n",
              "2     1337108  ...     a @ 415 b @ 425. tell your bro i say congrats!\n",
              "3     1560887  ...                           @littlefluffycat indeed.\n",
              "4     1466295  ...  completed race 4 life in 58mins with girlies f...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV_n2i0VbcnA"
      },
      "source": [
        "We are going to convert the target value to 1 instead of 4, in order to be more convenient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzvu6VnHGoQx"
      },
      "source": [
        "def change_to_1(x):\n",
        "    if (x == 4):\n",
        "        return 1\n",
        "    else:\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BS22SvMVGqkQ",
        "outputId": "e9d72d38-1a29-482f-a3b5-ef54f97b68b8"
      },
      "source": [
        "df['target'] = df['target'].apply(change_to_1)\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>target</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>flag</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>680949</td>\n",
              "      <td>0</td>\n",
              "      <td>2249621587</td>\n",
              "      <td>Fri Jun 19 22:41:08 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>sukumarpant</td>\n",
              "      <td>#brokenpromises...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>406741</td>\n",
              "      <td>0</td>\n",
              "      <td>2059003515</td>\n",
              "      <td>Sat Jun 06 16:03:21 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>MTMSparrow</td>\n",
              "      <td>david carradine so sad. thai's law not sure if...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1337108</td>\n",
              "      <td>1</td>\n",
              "      <td>2017466467</td>\n",
              "      <td>Wed Jun 03 08:26:14 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>itsmemcee</td>\n",
              "      <td>a @ 415 b @ 425. tell your bro i say congrats!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1560887</td>\n",
              "      <td>1</td>\n",
              "      <td>2186457254</td>\n",
              "      <td>Mon Jun 15 18:52:04 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>jdfreivald</td>\n",
              "      <td>@littlefluffycat indeed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1466295</td>\n",
              "      <td>1</td>\n",
              "      <td>2064458395</td>\n",
              "      <td>Sun Jun 07 06:19:20 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>CrazyHan</td>\n",
              "      <td>completed race 4 life in 58mins with girlies f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                               text\n",
              "0      680949  ...                                 #brokenpromises...\n",
              "1      406741  ...  david carradine so sad. thai's law not sure if...\n",
              "2     1337108  ...     a @ 415 b @ 425. tell your bro i say congrats!\n",
              "3     1560887  ...                           @littlefluffycat indeed.\n",
              "4     1466295  ...  completed race 4 life in 58mins with girlies f...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpm2iVwbHfui"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfOk_dUIHiko"
      },
      "source": [
        "## Spiltting the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32eJfTIvHmzI"
      },
      "source": [
        "In order to train our model, we are going to use a significant portion of the dataset, but we also need some data to test our classifier, thus we are going to split the dataset into 2 datasets: one for training, and one for testing.\n",
        "\n",
        "First, we are going to define which are our X and Y variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQ6FPChJQ0D"
      },
      "source": [
        "X = df['text']\n",
        "Y = df['target']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sd06AGPJUsm"
      },
      "source": [
        "Then, split the datasets by keeping 80% for training and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUG7PBDGJV-y"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwHyW7UvJXVk"
      },
      "source": [
        " ## Feature extraction: TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygOg69h-LdcC"
      },
      "source": [
        "Next up, we are going to transform our data to a feature vector. In order to do that, we're gonna use TfidfVectorizer.\n",
        "\n",
        "Because of the large amount of tweets, and thus the large amount of words, we are going to limit our vocabulary, in order to not create an erormus vector. We opt to store the vectors that occur more often in the tweets, in order to save space, while not compromising our accuracy. We are going words that occur on more than `0.5%` of the tweets, and on less than `70%` of the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE_ZOS2yngw-"
      },
      "source": [
        "tfidf_vec = TfidfVectorizer(max_df=0.7, min_df=0.005)  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaWZpy9gLj1W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "13b38c67-6fd1-4736-bf2c-4626e95aae96"
      },
      "source": [
        "tfidf_train_vec = tfidf_vec.fit_transform(X_train)\n",
        "tfidf_test_vec = tfidf_vec.transform(X_test)\n",
        "\n",
        "tfidf_train_data = pd.DataFrame(tfidf_train_vec.toarray())\n",
        "tfidf_test_data = pd.DataFrame(tfidf_test_vec.toarray())\n",
        "tfidf_train_data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>300</th>\n",
              "      <th>301</th>\n",
              "      <th>302</th>\n",
              "      <th>303</th>\n",
              "      <th>304</th>\n",
              "      <th>305</th>\n",
              "      <th>306</th>\n",
              "      <th>307</th>\n",
              "      <th>308</th>\n",
              "      <th>309</th>\n",
              "      <th>310</th>\n",
              "      <th>311</th>\n",
              "      <th>312</th>\n",
              "      <th>313</th>\n",
              "      <th>314</th>\n",
              "      <th>315</th>\n",
              "      <th>316</th>\n",
              "      <th>317</th>\n",
              "      <th>318</th>\n",
              "      <th>319</th>\n",
              "      <th>320</th>\n",
              "      <th>321</th>\n",
              "      <th>322</th>\n",
              "      <th>323</th>\n",
              "      <th>324</th>\n",
              "      <th>325</th>\n",
              "      <th>326</th>\n",
              "      <th>327</th>\n",
              "      <th>328</th>\n",
              "      <th>329</th>\n",
              "      <th>330</th>\n",
              "      <th>331</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.273519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.172694</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.219585</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.26365</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.509259</td>\n",
              "      <td>0.501088</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 332 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5    6    ...  325  326  327  328  329  330  331\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 332 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNlmDq3CtB6H"
      },
      "source": [
        "## Convert to Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVtiQP5DupZx"
      },
      "source": [
        "We are going to use the PyTorch API, thus we must convert all of our feature vectors to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY5kZoMhtNBq"
      },
      "source": [
        "train_x = tfidf_train_data.to_numpy()\n",
        "test_x = tfidf_test_data.to_numpy()\n",
        "\n",
        "X_train_tensor = torch.as_tensor(train_x, dtype=torch.float32)\n",
        "Y_train_tensor = torch.as_tensor(Y_train.values, dtype=torch.float32)\n",
        "\n",
        "X_test_tensor = torch.as_tensor(test_x, dtype=torch.float32)\n",
        "Y_test_tensor = torch.as_tensor(Y_test.values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "Y_train_tensor = Y_train_tensor.view(Y_train_tensor.size()[0], 1)\n",
        "Y_test_tensor = Y_test_tensor.view(Y_test_tensor.size()[0], 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYP_pCP289Zd"
      },
      "source": [
        "## First Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oON-JgH9Ath"
      },
      "source": [
        "For our first model we are going to use a simple neural network consisting of 1 fully connected layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w9gdQPvEuS-"
      },
      "source": [
        "We are going to do so by extending the extending the `nn.Module` class. This requires us to create an `__init__` and a `forward` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58K4CMLhO_fi"
      },
      "source": [
        "### Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85vUc9o-FBcX"
      },
      "source": [
        "class Model_1(nn.Module):\n",
        "  def __init__(self, input_size, hidden_neurons_first, hidden_neurons_second):\n",
        "    super(Model_1, self).__init__()\n",
        "\n",
        "    # 1st linear function: vocab_size -> hidden_neurons\n",
        "    self.layer_1 = nn.Linear(input_size, hidden_neurons_first)\n",
        "    # activation function for 1st layer\n",
        "    self.relu_1 = nn.ReLU()\n",
        "\n",
        "    # output layer: hidden_neurons -> 1 neuron (classification)\n",
        "    self.output = nn.Linear(hidden_neurons_second, 1)\n",
        "    # activation using the sigmoid function\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer_1(x)\n",
        "    out = self.relu_1(out)\n",
        "\n",
        "    out = self.output(out)\n",
        "    out = self.activation(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M3fW0jRO8rC"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51I3xKNWZE3B"
      },
      "source": [
        "Next up, we are going to define the appropriate functions that use the previosly mentioned class in order to train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AOYxUQlPc-N"
      },
      "source": [
        "#### Define the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QgCw4MiZeyQ"
      },
      "source": [
        "Code for training one epoch, which involves the learning procedure that we've learned from theory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Au5DD0OPwcJ"
      },
      "source": [
        "def train_epoch(optimizer, model, train_data, labels):\n",
        "  # clear gradients\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model.forward(train_data)\n",
        "  # compute loss w.r.t batch\n",
        "  # loss = nn.NLLLoss(outputs, labels)\n",
        "  loss = torch.nn.functional.mse_loss(outputs, labels)\n",
        "  # pass gradients back, startiing on loss value\n",
        "  loss.backward()\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  # return the total to keep track of how you did this time around\n",
        "  return loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCj3k0rj3VDn"
      },
      "source": [
        "#### Hyperparameters definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DstirNr4e9pX"
      },
      "source": [
        "Definition of our hyperparameters. Lots of different values were tried as I experemented with thoser parameters, in order to achieve the best results of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnFZBzV714KD"
      },
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "hidden_neurons_first = 200\n",
        "hidden_neurons_second = 200\n",
        "input_size = X_train_tensor.size()[1]\n",
        "model = Model_1(input_size, hidden_neurons_first, hidden_neurons_second)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGkfHgZm1mvh"
      },
      "source": [
        "#### Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-RUnTaFfMaL"
      },
      "source": [
        "The training procedure is simple: We train for every epoch that the user requested, and store the loss in order to plot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMwkaJa21v6D",
        "outputId": "06498973-8d0e-46ab-aa35-999fdb7c2b49"
      },
      "source": [
        "loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = train_epoch(optimizer, model, X_train_tensor, Y_train_tensor)\n",
        "  print(\"Epoch: \", epoch, \" loss: \", round(epoch_loss.item(),4))\n",
        "  loss.append(epoch_loss.item())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0  loss:  0.25\n",
            "Epoch:  1  loss:  0.2452\n",
            "Epoch:  2  loss:  0.2384\n",
            "Epoch:  3  loss:  0.2294\n",
            "Epoch:  4  loss:  0.2191\n",
            "Epoch:  5  loss:  0.209\n",
            "Epoch:  6  loss:  0.2006\n",
            "Epoch:  7  loss:  0.1945\n",
            "Epoch:  8  loss:  0.1904\n",
            "Epoch:  9  loss:  0.188\n",
            "Epoch:  10  loss:  0.1865\n",
            "Epoch:  11  loss:  0.1855\n",
            "Epoch:  12  loss:  0.1847\n",
            "Epoch:  13  loss:  0.184\n",
            "Epoch:  14  loss:  0.1834\n",
            "Epoch:  15  loss:  0.1829\n",
            "Epoch:  16  loss:  0.1825\n",
            "Epoch:  17  loss:  0.1822\n",
            "Epoch:  18  loss:  0.1818\n",
            "Epoch:  19  loss:  0.1814\n",
            "Epoch:  20  loss:  0.181\n",
            "Epoch:  21  loss:  0.1807\n",
            "Epoch:  22  loss:  0.1804\n",
            "Epoch:  23  loss:  0.1801\n",
            "Epoch:  24  loss:  0.1799\n",
            "Epoch:  25  loss:  0.1796\n",
            "Epoch:  26  loss:  0.1794\n",
            "Epoch:  27  loss:  0.1792\n",
            "Epoch:  28  loss:  0.1791\n",
            "Epoch:  29  loss:  0.1789\n",
            "Epoch:  30  loss:  0.1788\n",
            "Epoch:  31  loss:  0.1786\n",
            "Epoch:  32  loss:  0.1786\n",
            "Epoch:  33  loss:  0.1785\n",
            "Epoch:  34  loss:  0.1784\n",
            "Epoch:  35  loss:  0.1783\n",
            "Epoch:  36  loss:  0.1783\n",
            "Epoch:  37  loss:  0.1782\n",
            "Epoch:  38  loss:  0.1781\n",
            "Epoch:  39  loss:  0.178\n",
            "Epoch:  40  loss:  0.1779\n",
            "Epoch:  41  loss:  0.1779\n",
            "Epoch:  42  loss:  0.1778\n",
            "Epoch:  43  loss:  0.1777\n",
            "Epoch:  44  loss:  0.1776\n",
            "Epoch:  45  loss:  0.1775\n",
            "Epoch:  46  loss:  0.1774\n",
            "Epoch:  47  loss:  0.1773\n",
            "Epoch:  48  loss:  0.1772\n",
            "Epoch:  49  loss:  0.1771\n",
            "Epoch:  50  loss:  0.177\n",
            "Epoch:  51  loss:  0.1769\n",
            "Epoch:  52  loss:  0.1768\n",
            "Epoch:  53  loss:  0.1767\n",
            "Epoch:  54  loss:  0.1767\n",
            "Epoch:  55  loss:  0.1767\n",
            "Epoch:  56  loss:  0.1766\n",
            "Epoch:  57  loss:  0.1762\n",
            "Epoch:  58  loss:  0.1761\n",
            "Epoch:  59  loss:  0.1762\n",
            "Epoch:  60  loss:  0.176\n",
            "Epoch:  61  loss:  0.1757\n",
            "Epoch:  62  loss:  0.1757\n",
            "Epoch:  63  loss:  0.1756\n",
            "Epoch:  64  loss:  0.1753\n",
            "Epoch:  65  loss:  0.1752\n",
            "Epoch:  66  loss:  0.1752\n",
            "Epoch:  67  loss:  0.1749\n",
            "Epoch:  68  loss:  0.1748\n",
            "Epoch:  69  loss:  0.1747\n",
            "Epoch:  70  loss:  0.1745\n",
            "Epoch:  71  loss:  0.1744\n",
            "Epoch:  72  loss:  0.1743\n",
            "Epoch:  73  loss:  0.1741\n",
            "Epoch:  74  loss:  0.1739\n",
            "Epoch:  75  loss:  0.1738\n",
            "Epoch:  76  loss:  0.1737\n",
            "Epoch:  77  loss:  0.1735\n",
            "Epoch:  78  loss:  0.1734\n",
            "Epoch:  79  loss:  0.1732\n",
            "Epoch:  80  loss:  0.1731\n",
            "Epoch:  81  loss:  0.1729\n",
            "Epoch:  82  loss:  0.1728\n",
            "Epoch:  83  loss:  0.1726\n",
            "Epoch:  84  loss:  0.1724\n",
            "Epoch:  85  loss:  0.1723\n",
            "Epoch:  86  loss:  0.1721\n",
            "Epoch:  87  loss:  0.172\n",
            "Epoch:  88  loss:  0.1718\n",
            "Epoch:  89  loss:  0.1716\n",
            "Epoch:  90  loss:  0.1715\n",
            "Epoch:  91  loss:  0.1713\n",
            "Epoch:  92  loss:  0.1712\n",
            "Epoch:  93  loss:  0.171\n",
            "Epoch:  94  loss:  0.1709\n",
            "Epoch:  95  loss:  0.1707\n",
            "Epoch:  96  loss:  0.1705\n",
            "Epoch:  97  loss:  0.1704\n",
            "Epoch:  98  loss:  0.1702\n",
            "Epoch:  99  loss:  0.1701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "0BP5f_HdfVNw",
        "outputId": "29fe7825-e70b-48fa-e6d8-6846d65324a5"
      },
      "source": [
        "plt.plot(loss)\n",
        "plt.xlabel(\"Epochs\", fontsize=15)\n",
        "plt.ylabel(\"MSE Loss\", rotation=0, fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sg12ypSCi9A"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQtKzGS_f_dt"
      },
      "source": [
        "In order to predict our test set, we just apply the forwaring procedure with the weights that we have learned. The result will be a float number between 0 and 1, which is the probability of the item belonging to the positive class. Thus, in order to gather the predicted labels, we will round the number, as probabilities near 0 must be classified as negative, and near 1, must be classified as positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-flnmThNCiq9"
      },
      "source": [
        "predict_outs = model.forward(X_test_tensor)\n",
        "predict_Y = predict_outs.detach().numpy().round()\n",
        "test_Y = Y_test_tensor.numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNakcVqLgdzg"
      },
      "source": [
        "In order to check our results, we will print the accuracy of our model, as well as a full classification report for both of our classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWFVOSlLKaiy",
        "outputId": "2363acfe-aedd-43e7-fe37-bbf56c43af99"
      },
      "source": [
        "print(\"Accuracy score: {}\".format(round(accuracy_score(test_Y,predict_Y),3)))\n",
        "print(classification_report(test_Y,predict_Y))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.733\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.73      0.73     19922\n",
            "         1.0       0.73      0.74      0.74     20078\n",
            "\n",
            "    accuracy                           0.73     40000\n",
            "   macro avg       0.73      0.73      0.73     40000\n",
            "weighted avg       0.73      0.73      0.73     40000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}